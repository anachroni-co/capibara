Inicio R√°pido
============

Esta gu√≠a te ayudar√° a comenzar r√°pidamente con CapibaraGPT-v2, el modelo de lenguaje con **JAX nativo**, **Vector Quantization (VQ)**, y **optimizaciones TPU v4-32** completamente funcionales.

üèÜ **Estado del Proyecto**: **100% FUNCIONAL - SISTEMA COMPLETAMENTE OPERATIVO**

Instalaci√≥n R√°pida
------------------

.. code-block:: bash

    # Clonar el repositorio
    git clone https://github.com/user/CapibaraGPT-v2.git
    cd CapibaraGPT-v2
    
    # Instalar dependencias
    pip install -r requirements.txt
    
    # Instalaci√≥n en modo desarrollo
    pip install -e .

Verificaci√≥n de la Instalaci√≥n
-------------------------------

.. code-block:: python

    # Test de importaci√≥n principal - debe funcionar sin errores
    import capibara
    print("‚úÖ CapibaraGPT-v2 importado correctamente")
    
    # Verificar m√≥dulos principales
    from capibara.core import ModularCapibaraModel
    from capibara.config import ModularModelConfig
    from capibara.vq.vqbit import VQbitLayer
    print("‚úÖ M√≥dulos principales funcionales")

Configuraci√≥n Inicial
---------------------

CapibaraGPT-v2 utiliza configuraci√≥n TOML optimizada:

.. code-block:: python

    from capibara.config import ModularModelConfig
    
    # Configuraci√≥n desde archivo TOML (recomendado)
    config = ModularModelConfig.from_toml(
        "capibara/config/configs_toml/production/tpu_v4.toml"
    )
    
    # Configuraci√≥n program√°tica personalizada
    config = ModularModelConfig(
        model_name="capibara_lite_300m",
        hidden_size=768,
        num_layers=12,
        num_heads=12,
        vocab_size=32000,
        
        # JAX nativo y optimizaciones
        use_jax_native=True,
        use_tpu_v4_optimizations=True,
        
        # Vector Quantization
        use_vq=True,
        vq_codes=64,  # 64 para TPU v4, 128 para TPU v6
        
        # Sparsity y optimizaciones
        use_sparse=True,
        sparsity_ratio=0.65,
        mixed_precision=True
    )

Carga del Modelo Principal
--------------------------

.. code-block:: python

    from capibara.core import ModularCapibaraModel
    
    # Crear modelo con JAX nativo y optimizaciones TPU
    model = ModularCapibaraModel(config)
    
    # Inicializar modelo (configuraci√≥n autom√°tica de hardware)
    model.initialize()
    
    print(f"‚úÖ Modelo cargado: {model.config.model_name}")
    print(f"üîß JAX nativo: {model.config.use_jax_native}")
    print(f"üéØ VQ codes: {model.config.vq_codes}")
    print(f"‚ö° TPU v4 optimizations: {model.config.use_tpu_v4_optimizations}")

Generaci√≥n de Texto B√°sica
--------------------------

.. code-block:: python

    # Generaci√≥n b√°sica con optimizaciones autom√°ticas
    prompt = "Explica Vector Quantization en machine learning:"
    
    response = model.generate(
        prompt,
        max_length=150,
        temperature=0.7,
        use_vq=True,           # Usar Vector Quantization
        use_sparse=True,       # Activar sparsity autom√°tica
        tpu_optimized=True     # Usar optimizaciones TPU
    )
    
    print("ü§ñ Respuesta:", response)

Generaci√≥n Avanzada con VQ
--------------------------

.. code-block:: python

    from capibara.vq.vqbit import VQbitLayer
    
    # VQbit Layer para quantizaci√≥n avanzada
    vqbit = VQbitLayer(
        codebook_size=64,      # 64 c√≥digos para TPU v4
        embedding_dim=768,
        use_tpu_optimizations=True
    )
    
    # Generaci√≥n con VQ personalizado
    vq_response = model.generate_with_vq(
        prompt,
        vqbit_layer=vqbit,
        quantization_strength=0.8,
        adaptive_threshold=0.5
    )
    
    print("üéØ Respuesta VQ:", vq_response)

Entrenamiento y Fine-tuning
---------------------------

.. code-block:: python

    from capibara.training.unified_trainer import UnifiedTrainer
    from capibara.data import CapibaraDataset
    
    # Preparar datos
    dataset = CapibaraDataset.from_text_file("datos_entrenamiento.txt")
    
    # Configurar trainer con optimizaciones TPU
    trainer = UnifiedTrainer(
        model=model,
        dataset=dataset,
        batch_size=32,
        learning_rate=1e-4,
        use_tpu_v4=True,
        mixed_precision=True
    )
    
    # Entrenamiento con consensus distilling autom√°tico
    trainer.train(
        epochs=3,
        save_checkpoints=True,
        checkpoint_dir="./checkpoints"
    )

Agentes Inteligentes
-------------------

.. code-block:: python

    from capibara.agents import CapibaraAgentFactory
    
    # Crear agente especializado
    agent = CapibaraAgentFactory.create_agent(
        agent_type="research_assistant",
        model=model,
        personality="analytical",
        specialized_knowledge=["machine learning", "vector quantization"]
    )
    
    # Interacci√≥n con el agente
    response = agent.process_query(
        "¬øC√≥mo funciona la quantizaci√≥n vectorial en transformers?"
    )
    
    print("ü§ñ Agente:", response)

Monitoreo en Tiempo Real
------------------------

.. code-block:: python

    from capibara.monitoring import SystemMonitor
    
    # Monitor de sistema con m√©tricas TPU
    monitor = SystemMonitor(model)
    
    # Obtener m√©tricas en tiempo real
    metrics = monitor.get_real_time_metrics()
    
    print("üìä M√©tricas del sistema:")
    print(f"   üíæ Memoria TPU: {metrics['tpu_memory_usage']:.1f}%")
    print(f"   üî• Temperatura: {metrics['temperature']:.1f}¬∞C")
    print(f"   ‚ö° TFLOPS: {metrics['tflops']:.1f}")
    print(f"   üéØ VQ efficiency: {metrics['vq_efficiency']:.1f}%")
    print(f"   üí∞ Cost/hour: ${metrics['cost_per_hour']:.4f}")

Configuraci√≥n Multi-Plataforma
------------------------------

**TPU v4-32 (Recomendado)**

.. code-block:: python

    config = ModularModelConfig.from_toml(
        "capibara/config/configs_toml/production/tpu_v4.toml"
    )
    # 64 c√≥digos VQ, 275 TFLOPS, optimizaciones nativas

**ARM Axion (Cost-Effective)**

.. code-block:: python

    config = ModularModelConfig.from_toml(
        "capibara/config/configs_toml/specialized/arm_axion_inference.toml"
    )
    # 64 c√≥digos VQ, SVE vectorization, UMA memory

**GPU/CPU (Fallback)**

.. code-block:: python

    config = ModularModelConfig.from_toml(
        "capibara/config/configs_toml/development/development.toml"
    )
    # Fallback autom√°tico, optimizaciones disponibles

Debugging y Troubleshooting
---------------------------

.. code-block:: python

    # Verificar estado del sistema
    from capibara.utils import SystemDiagnostics
    
    diagnostics = SystemDiagnostics()
    
    # Check completo del sistema
    status = diagnostics.run_full_check()
    
    print("üîç Diagn√≥stico del sistema:")
    for component, status in status.items():
        icon = "‚úÖ" if status["healthy"] else "‚ùå"
        print(f"   {icon} {component}: {status['message']}")
    
    # Check espec√≠fico de imports
    import_status = diagnostics.check_imports()
    print(f"üì¶ Imports: {len(import_status['successful'])} OK, {len(import_status['failed'])} errores")

Ejemplos de Uso Avanzado
------------------------

**Generaci√≥n con Control de Estilo**

.. code-block:: python

    response = model.generate_with_style(
        prompt="Describe la cuantizaci√≥n vectorial",
        style="academic",           # 'academic', 'casual', 'technical'
        complexity_level=0.8,       # 0.0-1.0
        use_vq=True,
        temperature=0.6
    )

**Procesamiento por Lotes**

.. code-block:: python

    prompts = [
        "Explica VQ en t√©rminos simples",
        "Ventajas de TPU v4 vs GPU",
        "Diferencias entre sparsity y quantization"
    ]
    
    responses = model.generate_batch(
        prompts,
        max_length=100,
        use_vq=True,
        batch_size=8
    )

**Integraci√≥n con Meta Loop (Elixir/OTP)**

.. code-block:: python

    from capibara.meta_loop import CapibaraBridge
    
    # Bridge Python-Elixir para capacidades avanzadas
    bridge = CapibaraBridge(model)
    
    # RAG con control √©tico
    rag_response = bridge.query_with_rag(
        prompt,
        ethics_check=True,
        knowledge_base="ml_papers",
        confidence_threshold=0.8
    )

Guardar y Cargar Modelos
-----------------------

.. code-block:: python

    # Guardar modelo con configuraci√≥n
    model.save_pretrained(
        "./mi_modelo_vq",
        save_config=True,
        save_vq_codebooks=True,
        compress=True
    )
    
    # Cargar modelo guardado
    loaded_model = ModularCapibaraModel.from_pretrained(
        "./mi_modelo_vq",
        config_path="./mi_modelo_vq/config.toml"
    )

Optimizaciones de Rendimiento
-----------------------------

.. code-block:: python

    # Compilaci√≥n JIT para m√°ximo rendimiento
    model.compile_for_inference(
        input_shapes=[(1, 512)],   # Shapes t√≠picos
        optimization_level="aggressive",
        cache_compiled=True
    )
    
    # Warmup para TPU
    model.warmup_tpu(num_steps=10)
    
    # Configurar cache optimizado
    model.setup_optimized_cache(
        cache_size_gb=4.0,
        use_tpu_memory=True
    )

Siguientes Pasos
---------------

* Explora la :doc:`configuraci√≥n avanzada <configuration>`
* Revisa los :doc:`ejemplos espec√≠ficos <examples>`
* Consulta la :doc:`referencia de API <api/core_api>`
* √önete a nuestra comunidad para soporte t√©cnico
* Contribuye al desarrollo en GitHub

Recursos Adicionales
-------------------

- **Documentaci√≥n TPU v4**: :doc:`tpu_v4/optimizations`
- **Sistema VQ**: :doc:`layers/vq_layers`
- **JAX Nativo**: :doc:`jax/native_implementation`
- **Testing**: :doc:`testing/comprehensive_testing` 