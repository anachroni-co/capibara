Vector Quantization (VQ) - Sistema Avanzado
==========================================

CapibaraGPT-v2 implementa un **sistema Vector Quantization (VQ) avanzado** que proporciona quantizaci√≥n eficiente, optimizaciones hardware espec√≠ficas, y soporte para 64/128 c√≥digos de cuantizaci√≥n seg√∫n la plataforma disponible.

üèÜ **Estado**: **100% FUNCIONAL - SISTEMA VQ COMPLETAMENTE OPERATIVO**

Introducci√≥n a Vector Quantization
----------------------------------

Vector Quantization es una t√©cnica de compresi√≥n que representa vectores de alta dimensi√≥n usando un conjunto finito de vectores "c√≥digo" (codebook). En CapibaraGPT-v2, VQ permite:

- **Reducci√≥n de memoria**: Hasta 65% menos uso de memoria
- **Aceleraci√≥n de inferencia**: 2-3x m√°s r√°pido en TPU
- **Mejor generalizaci√≥n**: Regularizaci√≥n impl√≠cita
- **Eficiencia energ√©tica**: Menor consumo energ√©tico

VQbit Layer - Componente Principal
----------------------------------

**Uso B√°sico**

.. code-block:: python

    from capibara.vq.vqbit import VQbitLayer
    import capibara.jax as jax
    import capibara.jax.numpy as jnp
    
    # Crear VQbit Layer
    vqbit = VQbitLayer(
        codebook_size=64,           # 64 c√≥digos (TPU v4) o 128 (TPU v6)
        embedding_dim=768,          # Dimensi√≥n embeddings
        use_tpu_optimizations=True, # Optimizaciones TPU activas
        commitment_weight=0.25,     # Peso commitment loss
        diversity_regularization=True
    )
    
    # Forward pass con quantizaci√≥n
    input_embeddings = jnp.ones((32, 512, 768))  # [batch, seq, dim]
    
    quantized, indices, metrics = vqbit(input_embeddings)
    
    print(f"Input shape: {input_embeddings.shape}")
    print(f"Quantized shape: {quantized.shape}")
    print(f"Compression ratio: {metrics['compression_ratio']:.2f}")
    print(f"Codebook usage: {metrics['codebook_usage']:.1%}")

Optimizaciones TPU v4-32
------------------------

.. code-block:: python

    from capibara.vq.vqbit.tpu_optimizations import (
        TpuVqOptimizer,
        create_tpu_optimized_codebook
    )
    
    # Optimizador VQ espec√≠fico para TPU
    tpu_optimizer = TpuVqOptimizer(
        mesh_shape=(4, 8),            # TPU v4-32 mesh
        memory_limit_gb=32.0,         # L√≠mite memoria HBM
        use_bfloat16=True,            # Precisi√≥n nativa TPU
        async_updates=True            # Updates as√≠ncronos
    )

Comparaci√≥n: 64 vs 128 C√≥digos VQ
---------------------------------

**64 C√≥digos VQ (TPU v4, ARM Axion)**
- Estados cu√°nticos: 2^64 ‚âà 1.8 √ó 10^19
- Memoria eficiente: ~4GB HBM
- Velocidad: √ìptima para TPU v4-32
- Costo: Cost-effective para producci√≥n

**128 C√≥digos VQ (TPU v6 Enterprise)**
- Estados cu√°nticos: 2^128 ‚âà 3.4 √ó 10^38
- Memoria requerida: ~16GB HBM
- Velocidad: Requiere TPU v6 para eficiencia
- Costo: Premium enterprise (10-15x m√°s caro)

Integraci√≥n con Modelos
-----------------------

.. code-block:: python

    from capibara.core import ModularCapibaraModel
    from capibara.config import ModularModelConfig
    
    # Configuraci√≥n modelo con VQ
    config = ModularModelConfig(
        model_name="capibara_vq",
        hidden_size=768,
        num_layers=12,
        
        # Vector Quantization
        use_vq=True,
        vq_codes=64,
        vq_embedding_dim=768,
        vq_adaptive_threshold=0.5
    )
    
    # Crear modelo con VQ integrado
    model = ModularCapibaraModel(config)

Recursos y Referencias
---------------------

- **C√≥digo fuente VQ**: ``capibara/vq/``
- **Ejemplos**: ``examples/vector_quantization/``
- **API Reference**: :doc:`api/vq_api`
- **TPU Optimizations**: :doc:`tpu_v4/optimizations`
